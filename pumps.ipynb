{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting if water pumps need repair\n",
    "\n",
    "\n",
    "\n",
    "This code is a solution to the [Pump it Up: Data Mining the Water Table](https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/) competition. The goal of this competition is to take a rich dataset provided by [Taarifa](http://taarifa.org/) and [Tanzanian Ministry of Water](http://maji.go.tz/)\n",
    "\n",
    "It's worth noting, that while this file contains all the code you need to have a good offline model for this problem, it does not really explain the process of getting to this point. That could be a whole other post. But I could quickly mention that the process is much less linear than it looks in this post. \n",
    "\n",
    "In the post it looks pretty direct: load -> clean -> model -> tune -> Done drink a beer!\n",
    "But in reality its more like: load -> sample a tiny bit of data -> model -> clean -> model -> clean -> try with more data -> tune hyper params -> model -> clean -> and so on and so forth. So if it doesn't seem immediately obvious how to get to this point, it's because it isn't. Don't give up!\n",
    "\n",
    "Authors note: While you could spend days or even weeks trying to squeeze that last .001 percent of accuracy out of your model. The majority of useful learning occurs before you start squeezing the dry orange, so it's best to try to make your way to the top %1 of the competition and move on and learn from a new competition.\n",
    "\n",
    "### Sections\n",
    "* data cleaning\n",
    "* fitting a basic model\n",
    "* tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display  # Use this function to display more than one thing per cell.\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "pd.options.display.max_columns = None  # Show all the columns when we display a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a tool to reduce the dimensionality of categorical Data\n",
    "We do this so our data can fit in memory, but we don't totally have to throw categorical data out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REDUCE_DEFAULT = \"OTHER\"\n",
    "\n",
    "class FactorReducer():\n",
    "    \"\"\" Reduce a categorical column to a maximum number of factors. We'll keep the most commonly occuring values.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.factors_to_keep = []\n",
    "        \n",
    "    def fit(self, series, max_factors, reduce_to=REDUCE_DEFAULT):\n",
    "        self.factors_to_keep = series.value_counts().keys()[0:max_factors]\n",
    "        self.reduce_to = REDUCE_DEFAULT\n",
    "        \n",
    "    def transform(self, series):\n",
    "        new_column = series.copy(deep=True)\n",
    "        indices_for_other = np.logical_not(series.isin(self.factors_to_keep))\n",
    "        new_column.loc[indices_for_other] = self.reduce_to\n",
    "        return new_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(\"train_features.csv\")\n",
    "train_target = pd.read_csv(\"train_target.csv\").fillna(\"Empty\")\n",
    "test_features = pd.read_csv(\"test_features.csv\").fillna(\"Empty\")\n",
    "train = train_features.set_index(\"id\").join(train_target.set_index(\"id\"))  # Join into one training DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TARGET = \"status_group\"\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    'amount_tsh',\n",
    "    'gps_height',\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "    'population'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'construction_year',\n",
    "    'funder',\n",
    "    'installer',\n",
    "    'basin',\n",
    "    'region',\n",
    "    'public_meeting',\n",
    "    'scheme_management',\n",
    "    'permit',\n",
    "    'extraction_type',\n",
    "    'extraction_type_group',\n",
    "    'extraction_type_class',\n",
    "    'management',\n",
    "    'management_group',\n",
    "    'payment',\n",
    "    'payment_type',\n",
    "    'water_quality',\n",
    "    'quality_group',\n",
    "    'quantity',\n",
    "    'quantity_group',\n",
    "    'source',\n",
    "    'source_type',\n",
    "    'source_class',\n",
    "    'waterpoint_type',\n",
    "    'waterpoint_type_group'\n",
    "]\n",
    "\n",
    "DROP_COLUMNS = [\n",
    "    \"wpt_name\",\n",
    "    \"num_private\",\n",
    "    \"subvillage\",\n",
    "    \"region_code\",\n",
    "    \"district_code\",\n",
    "    \"lga\",\n",
    "    \"ward\",\n",
    "    \"recorded_by\",\n",
    "    \"scheme_name\",\n",
    "]\n",
    "\n",
    "CATEGORY_CAPS = {\n",
    "    \"funder\": 55,\n",
    "    \"installer\": 45,\n",
    "    \"construction_year\": 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/ipykernel/__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "train.drop(DROP_COLUMNS, 1)\n",
    "test_features.drop(DROP_COLUMNS, 1)\n",
    "\n",
    "# Make it so we can fit categorical data into memory.\n",
    "categorical_reducers = {}\n",
    "for feature, max_factors in CATEGORY_CAPS.items():\n",
    "    reducer = FactorReducer()\n",
    "    reducer.fit(train[feature], max_factors)\n",
    "    categorical_reducers[feature] = reducer\n",
    "\n",
    "def build_cleaned_frame(input_frame, numerics, categoricals, categorical_reducers):\n",
    "    category_frame = input_frame[categoricals]\n",
    "    for feature, reducer in categorical_reducers.items():\n",
    "        category_frame[feature] = reducer.transform(category_frame[feature])\n",
    "    cleaned_frame = input_frame[numerics]\n",
    "    dummy_category_frame = pd.get_dummies(category_frame, prefix=categoricals, prefix_sep=\"=\")\n",
    "    return cleaned_frame.join(dummy_category_frame)\n",
    "\n",
    "cleaned_train = build_cleaned_frame(train, NUMERIC_FEATURES, CATEGORICAL_FEATURES, categorical_reducers)\n",
    "cleaned_test = build_cleaned_frame(test_features, NUMERIC_FEATURES, CATEGORICAL_FEATURES, categorical_reducers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert date features into a more usable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def get_date_frame(df):\n",
    "    ret_frame = pd.DataFrame(index=df.index)\n",
    "\n",
    "    dates = pd.DatetimeIndex(df['date_recorded'])\n",
    "    ret_frame[\"year_recorded\"] = dates.year\n",
    "    ret_frame[\"month_recorded\"] = dates.month\n",
    "\n",
    "    dates = df['date_recorded'].astype('datetime64[ns]')\n",
    "    ret_frame[\"recorded_days_ago\"] = (datetime.date.today() - dates).apply(lambda x: x.days)\n",
    "    return ret_frame\n",
    "\n",
    "train_date_frame = get_date_frame(train)\n",
    "test_date_frame = get_date_frame(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index(['year_recorded', 'month_recorded', 'recorded_days_ago'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bf3cd7a75d43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcleaned_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaned_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_date_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcleaned_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cleaned_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcleaned_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaned_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_date_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcleaned_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cleaned_test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTARGET\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'YS.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   4383\u001b[0m         \u001b[0;31m# For SparseDataFrame's benefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4384\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[0;32m-> 4385\u001b[0;31m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[1;32m   4386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4387\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   4397\u001b[0m             return merge(self, other, left_on=on, how=how,\n\u001b[1;32m   4398\u001b[0m                          \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4399\u001b[0;31m                          suffixes=(lsuffix, rsuffix), sort=sort)\n\u001b[0m\u001b[1;32m   4400\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4401\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/pandas/tools/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[1;32m     37\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                          copy=copy, indicator=indicator)\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mmerge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_merge_doc\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m'\\nleft : DataFrame'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/pandas/tools/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         llabels, rlabels = items_overlap_with_suffix(ldata.items, lsuf,\n\u001b[0;32m--> 223\u001b[0;31m                                                      rdata.items, rsuf)\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mlindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mleft_indexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mitems_overlap_with_suffix\u001b[0;34m(left, lsuffix, right, rsuffix)\u001b[0m\n\u001b[1;32m   4443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlsuffix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4444\u001b[0m             raise ValueError('columns overlap but no suffix specified: %s' %\n\u001b[0;32m-> 4445\u001b[0;31m                              to_rename)\n\u001b[0m\u001b[1;32m   4446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4447\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlrenamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: columns overlap but no suffix specified: Index(['year_recorded', 'month_recorded', 'recorded_days_ago'], dtype='object')"
     ]
    }
   ],
   "source": [
    "cleaned_train = cleaned_train.join(train_date_frame)\n",
    "cleaned_train.to_csv('cleaned_train.csv', index=False)\n",
    "cleaned_test = cleaned_test.join(test_date_frame)\n",
    "cleaned_test.to_csv('cleaned_test.csv', index=False)\n",
    "train[TARGET].to_csv('YS.csv', index=False, header=True)\n",
    "cols = np.intersect1d(cleaned_train.columns, cleaned_test.columns) # ONLY use the columns that exist for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train[TARGET].to_csv('YS.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "amount_tsh                                     0\n",
       "basin=Internal                                 0\n",
       "basin=Lake Nyasa                               0\n",
       "basin=Lake Rukwa                               0\n",
       "basin=Lake Tanganyika                          0\n",
       "basin=Lake Victoria                            0\n",
       "basin=Pangani                                  0\n",
       "basin=Rufiji                                   0\n",
       "basin=Ruvuma / Southern Coast                  0\n",
       "basin=Wami / Ruvu                              0\n",
       "construction_year=0                            0\n",
       "construction_year=2000                         0\n",
       "construction_year=2003                         0\n",
       "construction_year=2006                         0\n",
       "construction_year=2007                         0\n",
       "construction_year=2008                         0\n",
       "construction_year=2009                         0\n",
       "construction_year=2010                         0\n",
       "construction_year=2011                         0\n",
       "construction_year=OTHER                        0\n",
       "extraction_type=afridev                        0\n",
       "extraction_type=cemo                           0\n",
       "extraction_type=climax                         0\n",
       "extraction_type=gravity                        0\n",
       "extraction_type=india mark ii                  0\n",
       "extraction_type=india mark iii                 0\n",
       "extraction_type=ksb                            0\n",
       "extraction_type=mono                           0\n",
       "extraction_type=nira/tanira                    0\n",
       "extraction_type=other                          0\n",
       "                                              ..\n",
       "source_class=unknown                           0\n",
       "source_type=borehole                           0\n",
       "source_type=dam                                0\n",
       "source_type=other                              0\n",
       "source_type=rainwater harvesting               0\n",
       "source_type=river/lake                         0\n",
       "source_type=shallow well                       0\n",
       "source_type=spring                             0\n",
       "water_quality=coloured                         0\n",
       "water_quality=fluoride                         0\n",
       "water_quality=fluoride abandoned               0\n",
       "water_quality=milky                            0\n",
       "water_quality=salty                            0\n",
       "water_quality=salty abandoned                  0\n",
       "water_quality=soft                             0\n",
       "water_quality=unknown                          0\n",
       "waterpoint_type=cattle trough                  0\n",
       "waterpoint_type=communal standpipe             0\n",
       "waterpoint_type=communal standpipe multiple    0\n",
       "waterpoint_type=dam                            0\n",
       "waterpoint_type=hand pump                      0\n",
       "waterpoint_type=improved spring                0\n",
       "waterpoint_type=other                          0\n",
       "waterpoint_type_group=cattle trough            0\n",
       "waterpoint_type_group=communal standpipe       0\n",
       "waterpoint_type_group=dam                      0\n",
       "waterpoint_type_group=hand pump                0\n",
       "waterpoint_type_group=improved spring          0\n",
       "waterpoint_type_group=other                    0\n",
       "year_recorded                                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Just a quick sanity check we didn't let any nonexistent data slip through\n",
    "cleaned_test[cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have clean data now, let's run a decent and fast classification model on it and see what kind of accuracy we can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all scores:  [ 0.75574426  0.763       0.763       0.75737869  0.77088544]\n",
      "mean:  0.762001677562\n",
      "CPU times: user 1.25 s, sys: 141 ms, total: 1.39 s\n",
      "Wall time: 1.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "SAMPLE_SIZE = 10000\n",
    "\n",
    "sample = cleaned_train.sample(SAMPLE_SIZE)\n",
    "sample_target = train[TARGET].loc[sample.index]\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "scores = cross_val_score(rfc, sample, sample_target, cv=5)  # (1)\n",
    "print(\"all scores: \", scores)\n",
    "print(\"mean: \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) `cross_val_score` splits our data up into several cross validation sets.\n",
    "For each set, we split the data into a training portion and a testing portion. \n",
    "As implied by the name, we train the model on the training portion, then we see how well it does on \n",
    "data it hasn't seen before using the testing portion. more on [cross validation](http://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we run an rfc on our whole dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all scores:  [ 0.79808097  0.79277839  0.79452862  0.79234007  0.79264186]\n",
      "mean:  0.794073980208\n",
      "CPU times: user 8.75 s, sys: 914 ms, total: 9.66 s\n",
      "Wall time: 9.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "scores = cross_val_score(rfc, cleaned_train, train[TARGET], cv=5)\n",
    "print(\"all scores: \", scores)\n",
    "print(\"mean: \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting, these are pretty solid results for not doing any model tuning. We just took our cleaned data and dropped a stock sklearn model on it. We could use XGBoost, but actually, it runs way slower on my machine. Maybe this faster run time will allow us to have bigger tuning wins.\n",
    "\n",
    "The best model in the competition is at .8285.\n",
    "\n",
    "There are a few cools things we can get from our `RandomForestClassifier` like a peek at feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.111561</td>\n",
       "      <td>latitude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.111560</td>\n",
       "      <td>longitude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.057815</td>\n",
       "      <td>quantity=dry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.056570</td>\n",
       "      <td>gps_height</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0.056486</td>\n",
       "      <td>recorded_days_ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.039908</td>\n",
       "      <td>population</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.031158</td>\n",
       "      <td>quantity_group=dry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.023556</td>\n",
       "      <td>extraction_type_class=other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.020316</td>\n",
       "      <td>extraction_type=other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.018073</td>\n",
       "      <td>month_recorded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017470</td>\n",
       "      <td>amount_tsh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.013445</td>\n",
       "      <td>quantity=enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.009392</td>\n",
       "      <td>waterpoint_type=communal standpipe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.009328</td>\n",
       "      <td>quantity_group=enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.008721</td>\n",
       "      <td>construction_year=OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.008577</td>\n",
       "      <td>funder=OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0.008030</td>\n",
       "      <td>waterpoint_type=other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.007543</td>\n",
       "      <td>payment_type=never pay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.007308</td>\n",
       "      <td>installer=OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.007198</td>\n",
       "      <td>funder=Government Of Tanzania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.007137</td>\n",
       "      <td>extraction_type_class=handpump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.006846</td>\n",
       "      <td>payment=never pay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.006603</td>\n",
       "      <td>quantity_group=insufficient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.006590</td>\n",
       "      <td>installer=DWE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.006363</td>\n",
       "      <td>permit=True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.006189</td>\n",
       "      <td>quantity=insufficient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.005615</td>\n",
       "      <td>waterpoint_type=communal standpipe multiple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.005578</td>\n",
       "      <td>waterpoint_type=hand pump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.005550</td>\n",
       "      <td>year_recorded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.005370</td>\n",
       "      <td>permit=False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.000131</td>\n",
       "      <td>funder=Kkkt_makwale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.000128</td>\n",
       "      <td>funder=Rc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.000121</td>\n",
       "      <td>installer=Distri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000121</td>\n",
       "      <td>extraction_type=windmill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000118</td>\n",
       "      <td>extraction_type=walimi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.000104</td>\n",
       "      <td>funder=Magadini-makiwaru Water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.000104</td>\n",
       "      <td>installer=WU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>funder=Shipo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.000096</td>\n",
       "      <td>installer=Gove</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>funder=Germany Republi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000087</td>\n",
       "      <td>extraction_type=cemo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.000080</td>\n",
       "      <td>installer=RC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.000079</td>\n",
       "      <td>funder=Rc Church</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0.000077</td>\n",
       "      <td>region=Dar es Salaam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.000071</td>\n",
       "      <td>installer=FinW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>installer=Da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>funder=Finw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.000058</td>\n",
       "      <td>management=other - school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.000046</td>\n",
       "      <td>funder=Fw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.000039</td>\n",
       "      <td>scheme_management=SWC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0.000032</td>\n",
       "      <td>waterpoint_type_group=dam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.000032</td>\n",
       "      <td>funder=Ces(gmbh)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>installer=ACRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>water_quality=fluoride abandoned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>installer=CES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>waterpoint_type=dam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>installer=Magadini-Makiwaru wa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>extraction_type=climax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>funder=Lawatefuka Water Supply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>installer=Lawatefuka water sup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>290 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     importance                                         name\n",
       "160    0.111561                                     latitude\n",
       "161    0.111560                                    longitude\n",
       "205    0.057815                                 quantity=dry\n",
       "113    0.056570                                   gps_height\n",
       "215    0.056486                            recorded_days_ago\n",
       "196    0.039908                                   population\n",
       "210    0.031158                           quantity_group=dry\n",
       "40     0.023556                  extraction_type_class=other\n",
       "29     0.020316                        extraction_type=other\n",
       "179    0.018073                               month_recorded\n",
       "0      0.017470                                   amount_tsh\n",
       "206    0.013445                              quantity=enough\n",
       "277    0.009392           waterpoint_type=communal standpipe\n",
       "211    0.009328                        quantity_group=enough\n",
       "19     0.008721                      construction_year=OTHER\n",
       "91     0.008577                                 funder=OTHER\n",
       "282    0.008030                        waterpoint_type=other\n",
       "189    0.007543                       payment_type=never pay\n",
       "145    0.007308                              installer=OTHER\n",
       "75     0.007198                funder=Government Of Tanzania\n",
       "38     0.007137               extraction_type_class=handpump\n",
       "180    0.006846                            payment=never pay\n",
       "212    0.006603                  quantity_group=insufficient\n",
       "125    0.006590                                installer=DWE\n",
       "195    0.006363                                  permit=True\n",
       "207    0.006189                        quantity=insufficient\n",
       "278    0.005615  waterpoint_type=communal standpipe multiple\n",
       "280    0.005578                    waterpoint_type=hand pump\n",
       "289    0.005550                                year_recorded\n",
       "194    0.005370                                 permit=False\n",
       "..          ...                                          ...\n",
       "83     0.000131                          funder=Kkkt_makwale\n",
       "97     0.000128                                    funder=Rc\n",
       "128    0.000121                             installer=Distri\n",
       "36     0.000121                     extraction_type=windmill\n",
       "35     0.000118                       extraction_type=walimi\n",
       "86     0.000104               funder=Magadini-makiwaru Water\n",
       "157    0.000104                                 installer=WU\n",
       "103    0.000102                                 funder=Shipo\n",
       "133    0.000096                               installer=Gove\n",
       "73     0.000091                       funder=Germany Republi\n",
       "21     0.000087                         extraction_type=cemo\n",
       "147    0.000080                                 installer=RC\n",
       "98     0.000079                             funder=Rc Church\n",
       "217    0.000077                         region=Dar es Salaam\n",
       "132    0.000071                               installer=FinW\n",
       "127    0.000063                                 installer=Da\n",
       "71     0.000063                                  funder=Finw\n",
       "164    0.000058                    management=other - school\n",
       "72     0.000046                                    funder=Fw\n",
       "241    0.000039                        scheme_management=SWC\n",
       "285    0.000032                    waterpoint_type_group=dam\n",
       "61     0.000032                             funder=Ces(gmbh)\n",
       "115    0.000030                               installer=ACRA\n",
       "270    0.000023             water_quality=fluoride abandoned\n",
       "117    0.000018                                installer=CES\n",
       "279    0.000014                          waterpoint_type=dam\n",
       "143    0.000014               installer=Magadini-Makiwaru wa\n",
       "22     0.000010                       extraction_type=climax\n",
       "84     0.000008               funder=Lawatefuka Water Supply\n",
       "142    0.000005               installer=Lawatefuka water sup\n",
       "\n",
       "[290 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(cleaned_train[cols], train[TARGET])\n",
    "pd.DataFrame({\n",
    "        'name': cols,\n",
    "        'importance': rfc.feature_importances_}).sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now we can quickly tell very interesting things like...the location of a well can pretty well indicate how likely it is to break down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "How could we not go there. Have you ever seen a kaggle forum?\n",
    "\n",
    "The random forest classifier is a pretty good model to use while we're still cleaning, exploring and playing with our dataset, because it runs fast. However, a well tuned xgboost usually performs better. Since we have all the features we want setup already, we can move to performance tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all scores:  [ 0.72677323  0.747       0.733       0.73636818  0.74437219]\n",
      "mean:  0.737502719392\n",
      "CPU times: user 1min 6s, sys: 477 ms, total: 1min 7s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "SAMPLE_SIZE = 10000\n",
    "\n",
    "sample = cleaned_train.sample(SAbMPLE_SIZE)\n",
    "sample_target = train[TARGET].loc[sample.index]\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "scores = cross_val_score(xgb, sample, sample_target, cv=5)  # (1)\n",
    "print(\"all scores: \", scores)\n",
    "print(\"mean: \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no tuning. XGB actually does worse than our random forest classifier. Let's tune this badboy up a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize xgboost and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  colsample_bytree |     gamma |   max_depth |   scale_pos_weight\n",
    "# 0.3124 |    1.7012 |     18.4428 |            20.0331\n",
    "xgb = XGBClassifier(max_depth=18, colsample_bytree=0.31, gamma=18)\n",
    "# xgb = XGBClassifier(max_depth=20, colsample_bytree=0.2)\n",
    "scores = cross_val_score(xgb, cleaned_train, train[TARGET])\n",
    "# scores = cross_val_score(xgb, sample, sample_target)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m--------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   criterion |   max_depth |   max_features | \n",
      "    1 | 00m05s | \u001b[35m   0.79160\u001b[0m | \u001b[32m     0.9572\u001b[0m | \u001b[32m    38.7470\u001b[0m | \u001b[32m        0.2406\u001b[0m | \n",
      "    2 | 00m03s |    0.70389 |      0.8487 |      4.0443 |         0.7075 | \n",
      "    3 | 00m07s | \u001b[35m   0.79357\u001b[0m | \u001b[32m     0.0031\u001b[0m | \u001b[32m    38.2414\u001b[0m | \u001b[32m        0.4993\u001b[0m | \n",
      "    4 | 00m02s |    0.73229 |      0.9330 |      7.3668 |         0.1477 | \n",
      "    5 | 00m07s |    0.79140 |      0.4597 |     34.0428 |         0.4425 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m--------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   criterion |   max_depth |   max_features | \n",
      "    6 | 00m09s |    0.79056 |      0.6137 |     49.3645 |         0.1554 | \n",
      "    7 | 00m05s |    0.79281 |      0.0000 |     25.1771 |         0.0500 | \n",
      "    8 | 00m06s |    0.78842 |      0.0000 |     41.2415 |         0.0500 | \n",
      "    9 | 00m15s |    0.79244 |      0.8671 |     23.3112 |         0.9500 | \n",
      "   10 | 00m15s |    0.78869 |      0.2462 |     37.7678 |         0.9500 | \n",
      "   11 | 00m05s |    0.79217 |      0.5624 |     20.7466 |         0.0500 | \n",
      "   12 | 00m06s |    0.79056 |      1.0000 |     36.1545 |         0.0500 | \n",
      "   13 | 00m06s |    0.79185 |      0.0000 |     30.6079 |         0.0500 | \n",
      "   14 | 00m11s |    0.79158 |      0.0454 |     37.8643 |         0.2328 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   15 | 00m16s |    0.79109 |      0.0000 |     50.0000 |         0.9500 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'funcalls': 52, 'grad': array([-0.00024807]), 'nit': 6, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH'}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16 | 00m07s |    0.79354 |      1.0000 |     26.3839 |         0.0500 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'funcalls': 49, 'grad': array([-0.00013715]), 'nit': 5, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH'}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   17 | 00m08s | \u001b[35m   0.79458\u001b[0m | \u001b[32m     0.6302\u001b[0m | \u001b[32m    22.8174\u001b[0m | \u001b[32m        0.0574\u001b[0m | \n",
      "   18 | 00m09s |    0.79074 |      0.0739 |     16.6335 |         0.2439 | \n",
      "   19 | 00m12s |    0.79322 |      0.1203 |     47.0269 |         0.3362 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'funcalls': 50, 'grad': array([  2.32846681e-05]), 'nit': 3, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH'}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20 | 00m07s |    0.78397 |      1.0000 |     16.4412 |         0.0607 | \n",
      "   21 | 00m11s |    0.79332 |      0.1074 |     39.2539 |         0.2444 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'funcalls': 50, 'grad': array([  3.89171485e-05]), 'nit': 3, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH'}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   22 | 00m18s |    0.79052 |      0.9029 |     46.1422 |         0.8308 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'funcalls': 52, 'grad': array([ 0.0004767]), 'nit': 4, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH'}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   23 | 00m16s |    0.79424 |      0.0779 |     20.5891 |         0.8428 | \n",
      "   24 | 00m09s | \u001b[35m   0.79604\u001b[0m | \u001b[32m     0.1281\u001b[0m | \u001b[32m    22.6534\u001b[0m | \u001b[32m        0.3791\u001b[0m | \n",
      "   25 | 00m16s |    0.79444 |      0.0878 |     21.8017 |         0.9203 | \n",
      "   26 | 00m08s |    0.79542 |      0.0460 |     22.2877 |         0.2397 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'funcalls': 62, 'grad': array([-0.00025091]), 'nit': 7, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH'}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   27 | 00m09s |    0.79234 |      0.8450 |     32.6161 |         0.2621 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'funcalls': 55, 'grad': array([  1.64822546e-05]), 'nit': 5, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH'}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   28 | 00m17s |    0.78891 |      0.8600 |     28.3482 |         0.9371 | \n",
      "   29 | 00m10s |    0.79473 |      0.1704 |     18.8833 |         0.4143 | \n",
      "   30 | 00m08s |    0.79557 |      0.1094 |     23.1440 |         0.1852 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'funcalls': 49, 'grad': array([ 0.00021756]), 'nit': 4, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH'}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   31 | 00m16s |    0.79227 |      0.0244 |     45.1178 |         0.8984 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'funcalls': 51, 'grad': array([-0.0002266]), 'nit': 6, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH'}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   32 | 00m09s |    0.79382 |      0.0863 |     26.7159 |         0.2230 | \n",
      "   33 | 00m14s |    0.79175 |      0.0065 |     47.4707 |         0.8602 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'funcalls': 52, 'grad': array([-0.00085913]), 'nit': 4, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH'}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   34 | 00m15s |    0.78926 |      0.4172 |     17.7807 |         0.9489 | \n",
      "   35 | 00m09s |    0.79557 |      0.0756 |     19.2232 |         0.3028 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-301:\n",
      "Process ForkPoolWorker-304:\n",
      "Process ForkPoolWorker-300:\n",
      "Process ForkPoolWorker-299:\n",
      "Process ForkPoolWorker-303:\n",
      "Process ForkPoolWorker-302:\n",
      "Process ForkPoolWorker-297:\n",
      "Process ForkPoolWorker-298:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 362, in get\n",
      "    return recv()\n",
      "  File \"/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "catching classes that do not inherit from BaseException is not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m   Traceback (most recent call last)",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m   Traceback (most recent call last)",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMemmapingPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    185\u001b[0m                           self._callback, self._args, self._kwargs)\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weakref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask_handler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0mtask_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2391fb42737d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     })\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mrfcBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;31m# Append most recently generated values to X and Y arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;31m# Updating the GP.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-2391fb42737d>\u001b[0m in \u001b[0;36mrfc_eval\u001b[0;34m(criterion, max_features, max_depth)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     )\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleaned_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTARGET\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                                               fit_params)\n\u001b[0;32m--> 140\u001b[0;31m                       for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    716\u001b[0m                     \u001b[0;31m# scheduling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;34m\"\"\"Shutdown the pool and restart a new one with the same parameters\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_ready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             self.configure(n_jobs=self.parallel.n_jobs, parallel=self.parallel,\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;34m\"\"\"Shutdown the process or thread pool\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiprocessingBackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# terminate does a join()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMemmapingPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mWindowsError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;31m# Workaround  occasional \"[Error 5] Access is denied\" issue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                 \u001b[0;31m# when trying to terminate a process under windows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: catching classes that do not inherit from BaseException is not allowed"
     ]
    }
   ],
   "source": [
    "from bayes_opt import bayesian_optimization\n",
    "\n",
    "def rfc_eval(criterion, max_features, max_depth):\n",
    "    \n",
    "    max_depth = int(max_depth)\n",
    "    crit = \"gini\" if criterion > .5 else \"entropy\"\n",
    "    xgbc = RandomForestClassifier(\n",
    "        criterion=crit,\n",
    "        max_features=max_features, \n",
    "        max_depth=max_depth \n",
    "    )\n",
    "    scores = cross_val_score(xgbc, cleaned_train, train[TARGET], n_jobs=-1)\n",
    "    return scores.mean()\n",
    "\n",
    "rfcBO = bayesian_optimization.BayesianOptimization(rfc_eval, {\n",
    "        'criterion': (0,1),\n",
    "        'max_features': (.05, .95),\n",
    "        'max_depth':(3,50)\n",
    "    })\n",
    "\n",
    "rfcBO.maximize(init_points=5, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_out = XGBClassifier(colsample_bytree=0.309, gamma=0.795, max_depth=18)\n",
    "xgb_out.fit(cleaned_train[cols], train[TARGET])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait, how did you get those params?\n",
    "I just ran bayesian optimization on smaller samples of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centered = cleaned_train[cols] - cleaned_train[cols].mean()\n",
    "centered = centered / centered.std()\n",
    "\n",
    "xgb_cross_val = XGBClassifier(colsample_bytree=0.309, gamma=0.795, max_depth=24)\n",
    "# scores = cross_val_score(xgb_cross_val, cleaned_train[cols], train[TARGET], cv=5, n_jobs=-1)\n",
    "scores = cross_val_score(xgb_cross_val, centered, train[TARGET], cv=5, n_jobs=-1)\n",
    "\n",
    "print(scores.mean())\n",
    "\n",
    "\"\"\"0.815336592481\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = xgb_out.predict(cleaned_test[cols])\n",
    "out_frame = pd.DataFrame({\n",
    "        \"id\": test_features[\"id\"],\n",
    "        \"status_group\": predictions\n",
    "    })\n",
    "out_frame.to_csv(\"bayes_opt_xgb_out.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=22)\n",
    "xgb = XGBClassifier(colsample_bytree=0.309, gamma=0.795, max_depth=18)\n",
    "\n",
    "voter = VotingClassifier(estimators=[\n",
    "        ('xgb', xgb),\n",
    "        ('rfc', rfc),\n",
    "    ], voting=\"soft\")\n",
    "scores = cross_val_score(voter, cleaned_train[cols], train[TARGET], cv=5, n_jobs=-1)\n",
    "print(scores.mean())\n",
    "voter.fit(cleaned_train[cols], train[TARGET])\n",
    "voter.predict(cleaned_test[cols])\n",
    "out_frame = pd.DataFrame({\n",
    "        \"id\": test_features[\"id\"],\n",
    "        \"status_group\": predictions\n",
    "    })\n",
    "out_frame.to_csv(\"voter_bayes_opt_xgb_rfc2.csv\", index=False)\n",
    "\n",
    "\"\"\"\n",
    "0.814865190752\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = voter.fit(cleaned_train[cols], train[TARGET])\n",
    "out_frame = pd.DataFrame({\n",
    "        \"id\": test_features[\"id\"],\n",
    "        \"status_group\": predictions\n",
    "    })\n",
    "out_frame.to_csv(\"voter_bayes_opt_xgb_rfc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a voter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Make a bunch of xgboosts\n",
    "COL = 'colsample_bytree'\n",
    "GAM = \"gamma\"\n",
    "MD  = 'max_depth'\n",
    "xgb_settings = [\n",
    "    {\n",
    "        COL:0.309,\n",
    "        GAM: 0.795,\n",
    "        MD: 18\n",
    "    },\n",
    "    {\n",
    "        COL:0.8892,\n",
    "        GAM: 0.3534,\n",
    "        MD: 12\n",
    "    },\n",
    "    {\n",
    "        COL:0.3356,\n",
    "        GAM: 2.6913,\n",
    "        MD: 9\n",
    "    },\n",
    "    {\n",
    "        COL:0.8892,\n",
    "        GAM: 0.3534,\n",
    "        MD: 12\n",
    "    },\n",
    "    {\n",
    "        COL:0.3575,\n",
    "        GAM: 0.9201,\n",
    "        MD: 19\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# rfc_most_deep = RandomForestClassifier(n_estimators=100, max_depth=30)\n",
    "# rfc_even_more_deep = RandomForestClassifier(n_estimators=100, max_depth=26)\n",
    "# rfc_more_deep = RandomForestClassifier(n_estimators=100, max_depth=22)\n",
    "# rfc_deep = RandomForestClassifier(n_estimators=100, max_depth=20)\n",
    "# rfc_shallow = RandomForestClassifier(n_estimators=100, max_depth=18)\n",
    "\n",
    "# rfc_estimators = [\n",
    "#         ('shallow_rfc', rfc_shallow),\n",
    "#         ('deep_rfc', rfc_deep),\n",
    "#         ('more_deep_rfc', rfc_more_deep),\n",
    "#         ('more_even_more_rfc', rfc_even_more_deep),\n",
    "#         ('more_most_rfc', rfc_most_deep),\n",
    "#     ]\n",
    "\n",
    "xgbs = {}\n",
    "for ii, xgb_setting in enumerate(xgb_settings):\n",
    "    xgbs[ii] = XGBClassifier(colsample_bytree=xgb_setting[COL], gamma=xgb_setting[GAM], max_depth=xgb_setting[MD])\n",
    "    \n",
    "# put them together with a voting classifier\n",
    "xgb_voter =  VotingClassifier(estimators=list(xgbs.items()), voting='soft')\n",
    "mixed_estimators = list(xgbs.items()) + rfc_estimators\n",
    "mixed_voter = VotingClassifier(estimators=mixed_estimators, voting=\"soft\")\n",
    "\n",
    "xgb_scores = cross_val_score(xgb_voter, cleaned_train[cols], train[TARGET], n_jobs=-1)\n",
    "mixed_scores = cross_val_score(mixed_voter, cleaned_train[cols], train[TARGET], n_jobs=-1)\n",
    "\n",
    "print(\"xgb voter: %s\" % xgb_scores.mean())\n",
    "print(\"mixed voter: %s\" % mixed_scores.mean())\n",
    "\n",
    "\"\"\"\n",
    "xgb voter: 0.808097643098\n",
    "mixed voter: 0.809646464646\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mixed_voter.fit(cleaned_train[cols], train[TARGET])\n",
    "predictions = voter.predict(cleaned_test[cols])\n",
    "output = pd.DataFrame({\n",
    "        'id': test_features['id'],\n",
    "        'status_group': predictions\n",
    "    })\n",
    "output.to_csv('mixed_voter_xgb_rfc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc_most_deep = RandomForestClassifier(n_estimators=100, max_depth=26)\n",
    "rfc_even_more_deep = RandomForestClassifier(n_estimators=100, max_depth=24)\n",
    "rfc_more_deep = RandomForestClassifier(n_estimators=100, max_depth=22)\n",
    "rfc_deep = RandomForestClassifier(n_estimators=100, max_depth=20)\n",
    "rfc_shallow = RandomForestClassifier(n_estimators=100, max_depth=18)\n",
    "voter = VotingClassifier(estimators=[\n",
    "        ('shallow_rfc', rfc_shallow),\n",
    "        ('deep_rfc', rfc_deep),\n",
    "        ('more_deep_rfc', rfc_more_deep),\n",
    "        ('more_even_more_rfc', rfc_even_more_deep),\n",
    "        ('more_most_rfc', rfc_most_deep),\n",
    "    ], voting=\"soft\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 40 0.747087542088 md=10\n",
    "# 100 0.794511784512 md=20\n",
    "# 40 0.798097643098 md=20\n",
    "# 30 0.795757575758 md=30\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_most_deep = RandomForestClassifier(n_estimators=100, max_depth=30)\n",
    "rfc_even_more_deep = RandomForestClassifier(n_estimators=100, max_depth=26)\n",
    "rfc_more_deep = RandomForestClassifier(n_estimators=100, max_depth=22)\n",
    "rfc_deep = RandomForestClassifier(n_estimators=100, max_depth=20)\n",
    "rfc_shallow = RandomForestClassifier(n_estimators=100, max_depth=18)\n",
    "\n",
    "scores_most_deep = cross_val_score(rfc_most_deep, cleaned_train[cols], train[TARGET], cv=5, n_jobs=-1)\n",
    "scores_even_more_deep = cross_val_score(rfc_even_more_deep, cleaned_train[cols], train[TARGET], cv=5, n_jobs=-1)\n",
    "scores_more_deep = cross_val_score(rfc_more_deep, cleaned_train[cols], train[TARGET], cv=5, n_jobs=-1)\n",
    "scores_deep = cross_val_score(rfc_deep, cleaned_train[cols], train[TARGET], cv=5, n_jobs=-1)\n",
    "scores_shallow = cross_val_score(rfc_shallow, cleaned_train[cols], train[TARGET], cv=5, n_jobs=-1)\n",
    "\n",
    "scores_voter = cross_val_score(voter, cleaned_train[cols], train[TARGET], n_jobs=-1, cv=5)\n",
    "print(\"shallow: %s\" % scores_shallow.mean())\n",
    "print(\"deep: %s\" % scores_deep.mean())\n",
    "print(\"more deep: %s\" % scores_more_deep.mean())\n",
    "print(\"even more deep: %s\" % scores_even_more_deep.mean())\n",
    "print(\"most deep: %s\" % scores_most_deep.mean())\n",
    "print(\"voter: %s\" % scores_voter.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bayes_opt import bayesian_optimization\n",
    "\n",
    "def eval_rfc(n_years, n_funders, n_installers):\n",
    "    train = loaded_train.copy(deep=True)\n",
    "    train.drop(DROP_COLUMNS, 1)\n",
    "    test_features.drop(DROP_COLUMNS, 1)\n",
    "\n",
    "    category_caps = {\n",
    "        \"funder\": int(n_funders),\n",
    "        \"installer\": int(n_installers),\n",
    "        \"construction_year\": int(n_years)\n",
    "    }\n",
    "    categorical_reducers = {}\n",
    "    for feature, max_factors in category_caps.items():\n",
    "        reducer = FactorReducer()\n",
    "        reducer.fit(train[feature], max_factors)\n",
    "        categorical_reducers[feature] = reducer\n",
    "        train[feature] = reducer.transform(train[feature])\n",
    "        \n",
    "    cleaned_train = build_cleaned_frame(train, NUMERIC_FEATURES, CATEGORICAL_FEATURES, categorical_reducers)\n",
    "    cleaned_test = build_cleaned_frame(test_features, NUMERIC_FEATURES, CATEGORICAL_FEATURES, categorical_reducers)\n",
    "    cleaned_train = cleaned_train.join(train_date_frame)\n",
    "    cleaned_test = cleaned_test.join(test_date_frame)\n",
    "    \n",
    "    rfc = RandomForestClassifier(n_estimators=20, max_depth=20)\n",
    "    scores = cross_val_score(rfc, cleaned_train[cleaned_test.columns], train[TARGET])\n",
    "    return scores.mean()\n",
    "\n",
    "bopt = bayesian_optimization.BayesianOptimization(eval_rfc, {\n",
    "        'n_years': (1, 30),\n",
    "        'n_funders': (1, 100),\n",
    "        'n_installers': (1, 100)\n",
    "    })\n",
    "\n",
    "bopt.maximize(init_points=5, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(train[TARGET])\n",
    "encoded_Y = le.transform(train[TARGET])\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train[TARGET].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centered = cleaned_train[cols] - cleaned_train[cols].mean()\n",
    "centered = centered / centered.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centered.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(centered, train[TARGET], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights = pd.get_dummies(y_train).sum().max() / pd.get_dummies(y_train).sum()\n",
    "cw = dict(zip(range(3), class_weights.values))\n",
    "cw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.regularizers import WeightRegularizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def keras_model():\n",
    "#     adam = Adam(lr=0.01)\n",
    "    model = Sequential() # make a model in a linear stack of layers\n",
    "    model.add(Dense(64, input_dim=290, activation=\"relu\"))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(output_dim=3, activation=\"softmax\"))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adagrad\", metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "kc = Sequential()\n",
    "kc.add(Dense(10, input_shape=(290,), activation=\"relu\"))\n",
    "kc.add(Dropout(.5))\n",
    "kc.add(Dense(3, activation='softmax'))\n",
    "kc.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# kc = KerasClassifier(keras_model)\n",
    "# kc = keras_model()\n",
    "kc.fit(X_train.as_matrix(), pd.get_dummies(y_train).as_matrix(), validation_split=0.2, nb_epoch=50, batch_size=32, class_weight=cw)\n",
    "\n",
    "predicted = kc.predict_classes(X_test.as_matrix())\n",
    "actual = y_test.map({\n",
    "        \"functional\": 0,\n",
    "        \"functional needs repair\": 1,\n",
    "        \"non functional\": 2\n",
    "    }).values\n",
    "\n",
    "\n",
    "print(pd.Series(predicted).value_counts())\n",
    "accuracy_score(predicted, actual)\n",
    "## Your input to keras needs to be input arrays, not dataframes.\n",
    "# scores = cross_val_score(kc, cleaned_train[cols].as_matrix(), pd.get_dummies(train[TARGET]).as_matrix(), cv=5, fit_params = {'nb_epoch': 3})\n",
    "# scores\n",
    "\n",
    "# train_test_split(cleaned_train[cols], pd.get_dummies(train[TARGET]), test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions2 = kc.predict_classes(cleaned_train[cols].as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1    34314\n",
    "2    20514\n",
    "0     4572\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "2    53609\n",
    "0     5791\n",
    "\"\"\"\n",
    "\n",
    "pd.Series(predictions2).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(np.argmax(predictions, axis=1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(np.argmax(predictions2, axis=1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(predictions).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tflearn\n",
    "# Classification\n",
    "tflearn.init_graph(num_cores=8, gpu_memory_fraction=0.5)\n",
    "\n",
    "net = tflearn.input_data(shape=[None, 290])\n",
    "net = tflearn.fully_connected(net, 64)\n",
    "net = tflearn.dropout(net, 0.5)\n",
    "net = tflearn.fully_connected(net, 3, activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "kc.fit(cleaned_train[cols].as_matrix(), pd.get_dummies(train[TARGET]).as_matrix(), validation_split=0.33, nb_epoch=3, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.744225589226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "/Users/David/anaconda/envs/sci/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:540: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.74473063973063969"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "logistic = LogisticRegression()\n",
    "print(cross_val_score(logistic, cleaned_train[cols], train[TARGET], n_jobs=-1).mean())\n",
    "n_estimators=10\n",
    "bagging = BaggingClassifier(logistic, max_samples=.2, n_estimators=n_estimators)\n",
    "cross_val_score(bagging, cleaned_train[cols], train[TARGET], n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "n_estimators=100\n",
    "svm = SVC(verbose=True, degree=5, kernel='poly')\n",
    "bagging = BaggingClassifier(svm, max_samples=1.0 / n_estimators, n_estimators=n_estimators)\n",
    "scores = cross_val_score(bagging, cleaned_train[cols], train[TARGET], n_jobs=-1)\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [sci]",
   "language": "python",
   "name": "Python [sci]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
